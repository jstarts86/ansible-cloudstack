---
- name: Kerberize Hadoop, Knox, Spark, and Livy
  hosts: all
  become: true
  vars:
    kerberos_realm: "CS2CLOUD.INTERNAL"
    kdc_server: "192.168.1.40"  # IP of KDCServer
    admin_principal: "admin/admin@CS2CLOUD.INTERNAL"
    admin_password: "{{ lookup('file', 'secure_files/ssl/password.txt') | default('') }}"
    hadoop_keytabs_path: "/home/ubuntu/keytabs"
    hadoop_user_principal: "hdfs/Knox@CS2CLOUD.INTERNAL"
    http_principal: "HTTP/Knox@CS2CLOUD.INTERNAL"
    livy_principal: "livy/Knox@CS2CLOUD.INTERNAL"
    knox_principal: "knox/Knox@CS2CLOUD.INTERNAL"
    yarn_principal: "yarn/Knox@CS2CLOUD.INTERNAL"
    hadoop_version: "hadoop-3.3.6"
    hadoop_home: "/usr/local/hadoop"
    java_home: "/usr/lib/jvm/java-1.11.0-openjdk-amd64"
    name_node_dir: "/home/ubuntu/data/nameNode"
    data_node_dir: "/home/ubuntu/data/dataNode"
    yarn_app_mapreduce_resource_mb: "512"
    yarn_app_map_resource_mb: "256"
    yarn_app_reduce_resource_mb: "256"
    replication_factor: "1"
    yarn_acl_enable: "0"
    yarn_nodemanager_resource_memory_mb: "2560"
    yarn_scheduler_maximum_allocation_mb: "2048"
    yarn_scheduler_minimum_allocation_mb: "512"
    yarn_nodemanager_vmem_check_enabled: "false"
    hadoop_security_authentication: "kerberos"
    hadoop_security_authorization_bool: "true"
    yarn_token_renew_interval: "3600000"
    yarn_token_max_lifetime: "604800000"
    namenode_hostname: "Master1"
    spark_version: "spark-3.4.3"
    spark_home: "/usr/local/spark"
    java11_home: "/usr/lib/jvm/java-11-openjdk-amd64"    # Java 11 path for keytool
    ssl_keystore_path: "/etc/ssl/hadoop/keystore.jks"
    ssl_truststore_path: "/etc/ssl/hadoop/truststore.jks"
    keystore_password: "{{ lookup('file', 'secure_files/ssl/password.txt') | default('') }}"
    ssl_cert_path: "/etc/ssl/hadoop"
    star_cert: "STAR.handong.edu.crt"
    star_key: "STAR.handong.edu.key"
    chain_cert: "chainca.crt"
    spark_history_provider: "org.apache.spark.deploy.history.FsHistoryProvider"
    spark_driver_memory: "6144m"
    spark_yarn_am_memory: "2048m"
    spark_executor_memory: "2560m"
    spark_executor_cores: "2"
    spark_dynamicAllocation_enable: "true"
    spark_dynamicAllocation_minExecutors: "2"
    spark_dynamicAllocation_maxExecutors: "4"
    spark_dynamicAllocation_initialExecutors: "2"
    spark_dynamicAllocation_executorIdleTimeout: "60s"
    spark_eventLog_enabled: "true"
    spark_history_fs_update_interval: "10s"
    spark_history_ui_port: "18080"
    spark_eventLog_dir: "hdfs://{{ namenode_hostname }}:9000/spark-logs"

  tasks:
    - name: Install Kerberos packages on all nodes
      apt:
        name:
          - krb5-user
          - krb5-kdc
        state: present
        update_cache: yes

    - name: Install krb5-admin-server on KDCServer
      apt:
        name: krb5-admin-server
        state: present
        update_cache: yes
      when: inventory_hostname == "KDCServer"

    - name: Configure krb5.conf
      template:
        src: "templates/kerb/krb5.conf.j2"
        dest: "/etc/krb5.conf"

    - name: Ensure ownership and permissions on Kerberos database
      file:
        path: "/var/lib/krb5kdc"
        owner: root
        group: root
        mode: "0700"
      when: inventory_hostname == "KDCServer"

    - name: Configure KDC on KDCServer
      when: inventory_hostname == "KDCServer"
      block:
        - name: Initialize the Kerberos database
          shell: kdb5_util create -s -r {{ kerberos_realm }}
          args:
            creates: "/var/lib/krb5kdc/principal"

        - name: Configure kadm5.acl
          copy:
            content: "{{ admin_principal }} *"
            dest: "/etc/krb5kdc/kadm5.acl"

        - name: Restart Kerberos services
          service:
            name: "{{ item }}"
            state: restarted
          loop:
            - krb5-admin-server
            - krb5-kdc

    - name: Add Hadoop service principals and create keytabs using kadmin.local
      shell: |
        kadmin.local -q "addprinc -randkey {{ hadoop_user_principal }}"
        kadmin.local -q "addprinc -randkey {{ http_principal }}"
        kadmin.local -q "addprinc -randkey {{ yarn_principal }}"
        kadmin.local -q "addprinc -randkey {{ livy_principal }}"
        kadmin.local -q "addprinc -randkey {{ knox_principal }}"
        kadmin.local -q "xst -k {{ hadoop_keytabs_path }}/hdfs.keytab {{ hadoop_user_principal }}"
        kadmin.local -q "xst -k {{ hadoop_keytabs_path }}/HTTP.keytab {{ http_principal }}"
        kadmin.local -q "xst -k {{ hadoop_keytabs_path }}/yarn.keytab {{ yarn_principal }}"
        kadmin.local -q "xst -k {{ hadoop_keytabs_path }}/livy.keytab {{ livy_principal }}"
        kadmin.local -q "xst -k {{ hadoop_keytabs_path }}/knox.keytab {{ knox_principal }}"
      when: inventory_hostname == "KDCServer"

    - name: Verify keytab creation
      stat:
        path: "{{ hadoop_keytabs_path }}/hdfs.keytab"
      register: hdfs_keytab_status
      when: inventory_hostname == "KDCServer"

    - name: Debug - Keytab Creation Status
      debug:
        msg: "Keytab file for hdfs was not created."
      when: "hdfs_keytab_status is defined and (not hdfs_keytab_status.stat.exists | default(false))"

    - name: Ensure hadoop_keytabs_path directory exists on all nodes
      file:
        path: "{{ hadoop_keytabs_path }}"
        state: directory
        owner: ubuntu
        mode: '0755'

- name: Copy keytabs to Master1 from KDCServer
  hosts: Master1
  become: true
  tasks:
    - name: Copy each keytab file from KDCServer
      copy:
        src: "{{ hadoop_keytabs_path }}/{{ item }}"
        dest: "{{ hadoop_keytabs_path }}/{{ item }}"
        owner: ubuntu
        mode: '0600'
        remote_src: yes
      loop:
        - hdfs.keytab
        - HTTP.keytab
        - yarn.keytab
        - livy.keytab
        - knox.keytab
      delegate_to: KDCServer

    - name: Copy keytabs to other nodes from KDCServer
      copy:
        src: "{{ hadoop_keytabs_path }}/{{ item }}"
        dest: "{{ hadoop_keytabs_path }}/{{ item }}"
        owner: ubuntu
        mode: '0600'
        remote_src: yes
      loop:
        - hdfs.keytab
        - HTTP.keytab
        - yarn.keytab
        - livy.keytab
        - knox.keytab
      delegate_to: KDCServer

    - name: Ensure keytab directory exists on all nodes
      file:
        path: "{{ hadoop_keytabs_path }}"
        state: directory
        owner: ubuntu
        mode: '0755'
    - name: Ensure hadoop configuration directory exists on all nodes
      file:
        path: "{{ hadoop_home }}/etc/hadoop"
        state: directory
        owner: ubuntu
        mode: '0755'

    - name: Configure Hadoop for Kerberos
      block:
        - name: Configure core-site.xml for Kerberos
          template:
            src: "templates/hadoop/core-site.xml.j2"
            dest: "{{ hadoop_home }}/etc/hadoop/core-site.xml"

        - name: Configure hdfs-site.xml for Kerberos
          template:
            src: "templates/hadoop/hdfs-site.xml.j2"
            dest: "{{ hadoop_home }}/etc/hadoop/hdfs-site.xml"

        - name: Deploy SSL client/server configuration
          template:
            src: "templates/hadoop/ssl-client.xml.j2"
            dest: "{{ hadoop_home }}/etc/hadoop/ssl-client.xml"
          with_items:
            - ssl-client.xml
            - ssl-server.xml

        - name: Configure Yarn for Kerberos
          template:
            src: "templates/hadoop/yarn-site.xml.j2"
            dest: "{{ hadoop_home }}/etc/hadoop/yarn-site.xml"

        - name: Configure MapReduce for Kerberos
          template:
            src: "templates/hadoop/mapred-site.xml.j2"
            dest: "{{ hadoop_home }}/etc/hadoop/mapred-site.xml"

    - name: Configure Spark for Kerberos
      block:
        - name: Configure spark-defaults.conf for Kerberos
          template:
            src: "templates/spark/spark-defaults.conf.j2"
            dest: "{{ spark_home }}/conf/spark-defaults.conf"

        - name: Enable Spark history server SSL
          template:
            src: "templates/spark/spark-history-server.xml.j2"
            dest: "{{ spark_home }}/conf/spark-history-server.xml"

    - name: Configure Livy for Kerberos
      template:
        src: "templates/livy/livy.conf.j2"
        dest: "{{ spark_home }}/conf/livy.conf"

    - name: Configure Knox for Kerberos
      template:
        src: "templates/knox/gateway-site.xml.j2"
        dest: "{{ spark_home }}/conf/gateway-site.xml"
    # - name: Restart Hadoop, Spark, Knox, and Livy services
    #   service:
    #     name: "{{ item }}"
    #     state: restarted
    #   loop:
    #     - hadoop-hdfs-namenode
    #     - hadoop-hdfs-datanode
    #     - hadoop-yarn-resourcemanager
    #     - hadoop-yarn-nodemanager
    #     - spark-history-server
    #     - livy-server
    #     - knox-gateway
    #
