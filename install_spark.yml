---
- name: Install and Configure Spark
  hosts: hadoop_namenode
  become: true
  vars:
    spark_version: "spark-3.4.3"
    spark_home: "/usr/local/spark"
    hadoop_home: "/usr/local/hadoop"
    java_home: "/usr/lib/jvm/java-1.11.0-openjdk-amd64"  # Ensure this matches the Java version used in Hadoop
    namenode_hostname: "Master1"
    spark_history_provider: "org.apache.spark.deploy.history.FsHistoryProvider"
    spark_driver_memory: "6144m"
    spark_yarn_am_memory: "2048m"
    spark_executor_memory: "2560m"
    spark_executor_cores: "2"
    spark_dynamicAllocation_enable: "true"
    spark_dynamicAllocation_minExecutors: "2"
    spark_dynamicAllocation_maxExecutors: "4"
    spark_dynamicAllocation_initialExecutors: "2"
    spark_dynamicAllocation_executorIdleTimeout: "60s"
    spark_eventLog_enabled: "true"
    spark_history_fs_update_interval: "10s"
    spark_history_ui_port: "18080"
    spark_eventLog_dir: "hdfs://{{ namenode_hostname }}:9000/spark-logs"
  tasks:
    - name: Download Spark
      get_url:
        url: "https://dlcdn.apache.org/spark/{{ spark_version }}/{{ spark_version }}-bin-hadoop3.tgz"
        dest: "/tmp/spark-{{ spark_version }}-bin-hadoop3.tgz"

    - name: Extract Spark
      unarchive:
        src: "/tmp/spark-{{ spark_version }}-bin-hadoop3.tgz"
        dest: "/usr/local"
        remote_src: yes
        creates: "/usr/local/{{ spark_version }}"

    - name: Rename Hadoop directory
      command: mv /usr/local/{{ spark_version }}-bin-hadoop3 /usr/local/spark
      
    - name: Ensure /etc/profile.d/ directory exists
      file:
        path: /etc/profile.d
        state: directory
        mode: '0755'


    - name: Set Spark environment variables
      template:
        src: "templates/spark/spark.sh.j2"
        dest: "/etc/profile.d/spark.sh"
        mode: '0755'

    - name: Template Spark configuration files
      template:
        src: "templates/spark/{{ item.src }}"
        dest: "{{ spark_home }}/conf/{{ item.dest }}"
      loop:
        - { src: 'spark-defaults.conf.j2', dest: 'spark-defaults.conf' }
        - { src: 'spark-env.sh.j2', dest: 'spark-env.sh' }

