---
- name: Update SSL configurations on Hadoop and Spark nodes
  hosts: all
  become: yes
  vars:
      name_node_dir: "/home/ubuntu/data/nameNode"
      data_node_dir: "/home/ubuntu/data/dataNode"
      spark_version: "spark-3.4.3"
      spark_home: "/usr/local/spark"
      hadoop_home: "/usr/local/hadoop"
      java_home: "/usr/lib/jvm/java-1.11.0-openjdk-amd64"  # Ensure this matches the Java version used in Hadoop
      namenode_hostname: "Master1"
      spark_history_provider: "org.apache.spark.deploy.history.FsHistoryProvider"
      spark_driver_memory: "6144m"
      spark_yarn_am_memory: "2048m"
      spark_executor_memory: "2560m"
      spark_executor_cores: "2"
      spark_dynamicAllocation_enable: "true"
      spark_dynamicAllocation_minExecutors: "2"
      spark_dynamicAllocation_maxExecutors: "4"
      spark_dynamicAllocation_initialExecutors: "2"
      spark_dynamicAllocation_executorIdleTimeout: "60s"
      spark_eventLog_enabled: "true"
      spark_history_fs_update_interval: "10s"
      spark_history_ui_port: "18080"
      spark_eventLog_dir: "hdfs://{{ namenode_hostname }}:9000/spark-logs"
      hadoop_version: "hadoop-3.3.6"
      yarn_app_mapreduce_resource_mb: "512"
      yarn_app_map_resource_mb: "256"
      yarn_app_reduce_resource_mb: "256"
      replication_factor: "1"
      yarn_acl_enable: "0"
      yarn_nodemanager_resource_memory_mb: "2560"
      yarn_scheduler_maximum_allocation_mb: "2048"
      yarn_scheduler_minimum_allocation_mb: "512"
      yarn_nodemanager_vmem_check_enabled: "false"
      hadoop_security_authentication: "kerberos"
      hadoop_security_authorization_bool: "true"
      yarn_token_renew_interval: "3600000"
      yarn_token_max_lifetime: "604800000"
  tasks:
    - name: Ensure certs directory exists
      file:
        path: "{{ ssl_keystore_path }}"
        state: directory
        owner: ubuntu
        mode: '0700'

    - name: Copy SSL certificate files to each host
      copy:
        src: "{{ item }}"
        dest: "{{ ssl_keystore_path }}/"
        owner: ubuntu
        mode: '0600'
      with_items:
        - "secure_files/ssl/STAR.handong.edu.crt"
        - "secure_files/ssl/STAR.handong.edu.key"
        - "secure_files/ssl/chainca.crt"

    - name: Configure HDFS for SSL
      template:
        src: templates/hadoop/hdfs-site.xml.j2
        dest: "/usr/local/hadoop/etc/hadoop/hdfs-site.xml"
      when: "'hadoop_cluster' in group_names"

    - name: Deploy SSL client configuration for HDFS
      template:
        src: templates/hadoop/ssl-client.xml.j2
        dest: "/usr/local/hadoop/etc/hadoop/ssl-client.xml"
      when: "'hadoop_cluster' in group_names"

    - name: Deploy SSL server configuration for HDFS
      template:
        src: templates/hadoop/ssl-server.xml.j2
        dest: "/usr/local/hadoop/etc/hadoop/ssl-server.xml"
      when: "'hadoop_cluster' in group_names"

    - name: Configure Spark for SSL
      template:
        src: templates/spark/spark-defaults.conf.j2
        dest: "{{ spark_home }}/conf/spark-defaults.conf"
      when: "'spark_cluster' in group_names"

    - name: Configure Livy for SSL
      template:
        src: templates/livy/livy.conf.j2
        dest: "/opt/livy/conf/livy.conf"
      when: "'livy' in group_names"

    - name: Configure Knox for SSL
      template:
        src: templates/knox/gateway-site.xml.j2
        dest: "{{ knox_home }}/conf/gateway-site.xml"
      when: "'knox' in group_names"

  # handlers:
  #   - name: restart_hdfs
  #     service:
  #       name: hadoop-hdfs-namenode
  #       state: restarted
  #
  #   - name: restart_spark
  #     service:
  #       name: spark-master
  #       state: restarted
  #
  #   - name: restart_livy
  #     service:
  #       name: livy-server
  #       state: restarted
  #
  #   - name: restart_knox
  #     service:
  #       name: knox-gateway
  #       state: restarted
